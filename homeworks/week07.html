<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Probability: Interpretations, Axioms, and Measure Theory</title>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      background: #f7f7f7;
      color: #222;
    }
    .page {
      max-width: 800px;
      margin: 40px auto;
      padding: 24px;
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 1px 3px rgba(0,0,0,.06);
    }
    h1, h2, h3 {
      font-weight: 600;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 1.8rem; margin-top: 0; }
    h2 { font-size: 1.35rem; }
    h3 { font-size: 1.1rem; }
    p { line-height: 1.6; margin: 0.6em 0; }
    ul { margin: 0.4em 0 0.4em 1.4em; padding: 0; }
  </style>
</head>

<body>
  <main class="page">

    <h1>Probability Theory: Interpretations, Axioms, and Measure Theory</h1>

    <p>
      Probability has several interpretations, classical, frequentist, Bayesian, geometric, and historically these
      led to conceptual disagreements. The modern axiomatic approach provides a unifying framework that all
      interpretations satisfy. Measure theory then offers a precise language in which probability spaces and random
      variables can be rigorously defined.
    </p>

    <h2>1. Interpretations of Probability</h2>

    <h3>1.1 Classical Interpretation</h3>
    <p>
      Based on symmetry and equally likely outcomes. For a finite sample space,
      \[
        P(A)=\frac{\text{favorable outcomes}}{\text{possible outcomes}}.
      \]
    </p>

    <h3>1.2 Frequentist Interpretation</h3>
    <p>
      Probability is identified with a long-run relative frequency under repeated trials.
      It struggles with one-off events and the idealization of infinite repetition.
    </p>

    <h3>1.3 Bayesian Interpretation</h3>
    <p>
      Probability represents rational degrees of belief. Bayes’ rule updates beliefs:
      \[
        P(H\mid D)=\frac{P(D\mid H)P(H)}{P(D)}.
      \]
    </p>

    <h3>1.4 Geometric Interpretation</h3>
    <p>
      Probability is proportional to geometric measure:
      \[
        P(A)=\frac{\text{measure}(A)}{\text{measure}(R)}.
      \]
    </p>

    <h3>1.5 How the Axioms Reconcile These Views</h3>
    <p>
      Kolmogorov’s axioms require only that \(P\) is a function on a σ-algebra satisfying:
    </p>
    <ul>
      <li>\(P(A)\ge 0\)</li>
      <li>\(P(\Omega)=1\)</li>
      <li>For disjoint events \(A_i\):
        \[
          P\Bigl(\bigcup_i A_i\Bigr)=\sum_i P(A_i).
        \]
      </li>
    </ul>
    <p>
      Classical, frequentist, Bayesian, and geometric interpretations all produce probability measures
      that satisfy these axioms, even if their philosophical motivations differ.
    </p>

    <h2>2. Probability and Measure Theory</h2>

    <h3>2.1 Sigma-algebras</h3>
    <p>
      A σ-algebra \(\mathcal F\) on \(\Omega\) is a collection of subsets (events) such that:
    </p>
    <ul>
      <li>\(\Omega \in \mathcal F\)</li>
      <li>If \(A \in \mathcal F\) then \(A^c \in \mathcal F\)</li>
      <li>If \(A_1, A_2, \dots \in \mathcal F\), then \(\bigcup_{i=1}^\infty A_i \in \mathcal F\)</li>
    </ul>

    <h3>2.2 Probability Measures</h3>
    <p>
      A probability measure is a function \(P:\mathcal F\to[0,1]\) with
      \[
        P(\Omega)=1,
      \]
      and for pairwise disjoint \(A_i\),
      \[
        P\Bigl(\bigcup_{i=1}^\infty A_i\Bigr)=\sum_{i=1}^\infty P(A_i).
      \]
      The triple \((\Omega,\mathcal F,P)\) is a probability space.
    </p>

    <h3>2.3 Measurable Functions and Random Variables</h3>
    <p>
      A random variable is a measurable function \(X:\Omega\to\mathbb R\), meaning
      \[
        X^{-1}(B)\in\mathcal F \quad \text{for all Borel sets } B.
      \]
      Its distribution is
      \[
        P_X(B)=P(X\in B)=P(X^{-1}(B)),
      \]
      and its expectation is the Lebesgue integral
      \[
        \mathbb{E}[X]=\int_\Omega X\,dP.
      \]
    </p>

    <h2>3. Consequences of the Axioms</h2>

    <h3>3.1 Subadditivity</h3>
    <p>
      For any events \(A_1,A_2,\dots\),
      \[
        P\Bigl(\bigcup_i A_i\Bigr)\le \sum_i P(A_i).
      \]
    </p>

    <p><strong>Proof.</strong> Define disjoint sets
      \[
      \begin{aligned}
        B_1 &= A_1,\\
        B_2 &= A_2 \setminus A_1,\\
        B_3 &= A_3 \setminus (A_1 \cup A_2),\\
        &\;\vdots \\
        B_n &= A_n \setminus \bigcup_{k=1}^{n-1} A_k.
      \end{aligned}
      \]
      Then \(B_i \subseteq A_i\), the \(B_i\) are disjoint, and
      \[
        \bigcup_{i=1}^\infty A_i = \bigcup_{i=1}^\infty B_i.
      \]
      By countable additivity,
      \[
        P\Bigl(\bigcup_i A_i\Bigr)
        = P\Bigl(\bigcup_i B_i\Bigr)
        = \sum_i P(B_i)
        \le \sum_i P(A_i).
      \]
    </p>

    <h3>3.2 Inclusion–Exclusion (two sets)</h3>
    <p>
      For two events \(A,B\),
      \[
        P(A\cup B)=P(A)+P(B)-P(A\cap B).
      \]
    </p>

    <h3>3.3 General Finite Inclusion–Exclusion</h3>

    <div>
    $$
    \displaystyle
    P\left(\bigcup_{i=1}^{n} A_i\right)
    =
    \sum_{i=1}^{n} P(A_i)
    \;-\;
    \sum_{1\le i<j\le n} P(A_i \cap A_j)
    \;+\;
    \sum_{1\le i<j<k\le n} P(A_i \cap A_j \cap A_k)
    \;-\;\cdots\;+\;
    (-1)^{\,n-1} P(A_1 \cap \cdots \cap A_n).
    $$
    </div>

    <p>
      This can be shown by induction on \(n\), using the axioms and set identities. The alternating sums correct for
      over-counting of intersections when we simply add the probabilities of individual events.
    </p>

    <h2>4. Simulating a Counting Process and the Poisson Limit</h2>

    <h3>4.1 Simulation Setup</h3>
    <p>
      Consider a time interval \([0,T]\), for example \(T=1\), and a constant rate parameter \(\lambda &gt; 0\).
      To simulate random “successes” occurring independently and uniformly in time:
    </p>
    <ul>
      <li>Divide \([0,T]\) into \(n\) equal subintervals of length \(\Delta t = T/n\).</li>
      <li>In each subinterval, generate an event with probability \(p = \lambda\,\Delta t = \lambda T/n\).</li>
      <li>Let \(N_n(t)\) be the number of events observed up to time \(t\), i.e. counting how many of these Bernoulli
        trials (subintervals) up to \(t\) produced a success.
      </li>
    </ul>
    <p>
      For a fixed \(t \in [0,T]\), the number of subintervals up to time \(t\) is approximately
      \[
        k \approx \frac{nt}{T},
      \]
      and \(N_n(t)\) is then a binomial random variable
      \[
        N_n(t) \sim \text{Binomial}\bigl(k,\; p=\lambda T/n\bigr).
      \]
    </p>

    <h3>4.2 The Limiting Stochastic Process</h3>
    <p>
      As \(n \to \infty\), \(\Delta t \to 0\) and \(p = \lambda\,\Delta t \to 0\) while the expected number of events
      in \([0,t]\) remains of order \(\lambda t\). In this regime, the binomial distribution converges to a Poisson
      distribution:
      \[
        N_n(t) \;\xrightarrow[n\to\infty]{d}\; N(t),
      \]
      where
      \[
        N(t) \sim \text{Poisson}(\lambda t).
      \]
      The process \( \{N(t)\}_{t\ge 0} \) obtained in the limit is a
      <strong>Poisson process with rate \(\lambda\)</strong>.
    </p>

    <p>
      More precisely, the limiting process \(N(t)\) is characterized by:
    </p>
    <ul>
      <li><strong>Independent increments:</strong> for disjoint time intervals, the counts of events are independent.</li>
      <li><strong>Stationary increments:</strong> for \(s &lt; t\),
        \[
          N(t)-N(s) \sim \text{Poisson}(\lambda (t-s)),
        \]
        so the distribution depends only on the length of the interval.
      </li>
      <li><strong>Right-continuous, step function paths:</strong> \(N(t)\) jumps by 1 at random event times and
        is constant between jumps.
      </li>
      <li><strong>No simultaneous events:</strong> in the limit, the probability of two or more events in a tiny
        interval is negligible compared to the probability of one event.
      </li>
    </ul>

    <h3>4.3 Inter-arrival Times and the Role of \(\lambda\)</h3>
    <p>
      Let \(S_1, S_2, \dots\) be the event times, and define inter-arrival times
      \[
        X_1 = S_1,\quad X_2 = S_2 - S_1,\quad X_3 = S_3 - S_2,\dots
      \]
      In a Poisson process with rate \(\lambda\), these \(X_i\) are independent and identically distributed with
      exponential distribution:
      \[
        X_i \sim \text{Exponential}(\lambda),\quad \mathbb{P}(X_i &gt; x) = e^{-\lambda x}, \; x \ge 0.
      \]
      The parameter \(\lambda\) has several equivalent meanings:
    </p>
    <ul>
      <li>Average rate of events per unit time.</li>
      <li>For small \(\Delta t\),
        \[
          \mathbb{P}(\text{one event in } \Delta t) \approx \lambda\,\Delta t.
        \]
      </li>
      <li>Inverse of the mean inter-arrival time:
        \[
          \mathbb{E}[X_i] = \frac{1}{\lambda}.
        \]
      </li>
    </ul>

    <p>
      The simulation scheme, many small time bins, each with an independent event of probability \(\lambda \Delta t\), 
      thus approximates a Poisson process. As the mesh of the partition gets finer (\(n\) large), the discrete counting
      process \(N_n(t)\) converges in distribution to the continuous-time Poisson process with rate \(\lambda\).
    </p>

  </main>
</body>
</html>
